# -*- coding: utf-8 -*-
"""streamlit_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kjE3Fr2DYEBrgZuhJWMTvEMzevd_OdYc
"""

# streamlit_app.py
import streamlit as st
import pandas as pd
import numpy as np
import os
import pickle
from sentence_transformers import SentenceTransformer
import faiss
from sklearn.preprocessing import normalize
from typing import List, Dict, Any
import time
import re
import openai
from dotenv import load_dotenv

load_dotenv()
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
if OPENAI_KEY:
    openai.api_key = OPENAI_KEY

st.set_page_config(page_title="Property RAG", layout="wide")

# -------------------------
# Helper functions
# -------------------------
@st.cache_resource(show_spinner=False)
def get_embed_model(name="all-MiniLM-L6-v2"):
    return SentenceTransformer(name)

def preprocess(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    # normalize column names
    df.columns = [c.strip().lower() for c in df.columns]
    # ensure important columns exist
    # unify textual description column
    if "property_type_full_description" in df.columns:
        df["description"] = df["property_type_full_description"].fillna("") + " | " + df.get("type","").fillna("").astype(str)
    else:
        df["description"] = df.get("description", "").fillna("") + " | " + df.get("type","").fillna("").astype(str)

    # fillna for numeric fields
    for c in ["price","bedrooms","bathrooms","crime_score_weight"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    # keep index mapping
    df.reset_index(drop=True, inplace=True)
    return df

def build_faiss_index(embeddings: np.ndarray, dim: int):
    index = faiss.IndexFlatIP(dim)  # inner product; we'll normalize vectors to use cosine
    faiss.normalize_L2(embeddings)
    index.add(embeddings)
    return index

def save_index_and_meta(index, meta_list, filepath_index="faiss.index", filepath_meta="meta.pkl"):
    faiss.write_index(index, filepath_index)
    with open(filepath_meta, "wb") as f:
        pickle.dump(meta_list, f)

def load_index_and_meta(filepath_index="faiss.index", filepath_meta="meta.pkl"):
    if not (os.path.exists(filepath_index) and os.path.exists(filepath_meta)):
        return None, None
    index = faiss.read_index(filepath_index)
    with open(filepath_meta, "rb") as f:
        meta_list = pickle.load(f)
    return index, meta_list

def embed_texts(model, texts: List[str]) -> np.ndarray:
    emb = model.encode(texts, show_progress_bar=False, convert_to_numpy=True)
    # normalize
    emb = normalize(emb, norm='l2')
    return emb.astype("float32")

def retrieve(index, query_embedding: np.ndarray, k=5):
    # query_embedding must be normalized
    faiss.normalize_L2(query_embedding)
    D, I = index.search(query_embedding, k)
    return D, I

# Light natural-language query dispatch (rule-based for common analytics)
def handle_analytic_query(df: pd.DataFrame, q: str):
    q_l = q.lower()
    # avg price of X-bedroom
    m = re.search(r"average price of (\d+)[ -]?bed", q_l)
    if m:
        beds = int(m.group(1))
        sub = df[df["bedrooms"] == beds]
        if len(sub):
            return f"Average price of {beds}-bed homes (based on {len(sub)} records): ${sub['price'].mean():,.2f}", sub
        else:
            return f"No records found for {beds}-bed homes.", pd.DataFrame()

    # find properties under $X with Y+ bathrooms
    m = re.search(r"under \$?([0-9,]+)\s*(k|m)?\s*.*?(\d+)\+?\s*bath", q_l)
    if m:
        raw = int(m.group(1).replace(",",""))
        mult = m.group(2)
        if mult == "k":
            cap = raw*1000
        elif mult == "m":
            cap = raw*1_000_000
        else:
            cap = raw
        baths = int(m.group(3))
        sub = df[(df["price"] <= cap) & (df["bathrooms"] >= baths)]
        return f"Found {len(sub)} properties with <= ${cap:,} and >= {baths} bathrooms.", sub

    # which area has the most crime?
    if "most crime" in q_l or "most crimes" in q_l or "which area has the most crime" in q_l:
        if "crime_score_weight" in df.columns:
            tmp = df.groupby("address", dropna=True)["crime_score_weight"].mean().sort_values(ascending=False)
            if len(tmp):
                top = tmp.index[0]
                return f"Area with highest average crime score: {top} (avg score {tmp.iloc[0]:.2f})", df[df["address"] == top]
            else:
                return "No crime score data available in dataset.", pd.DataFrame()
        else:
            return "Crime score column not present in the dataset.", pd.DataFrame()

    # Compare prices between studio and 2 bed
    if "compare prices" in q_l and "studio" in q_l and "2" in q_l:
        studio = df[df["type"].str.contains("studio", case=False, na=False)]
        two = df[df["bedrooms"] == 2]
        summary = {}
        summary["studio_count"] = len(studio)
        summary["studio_avg"] = studio["price"].mean() if len(studio) else None
        summary["2bed_count"] = len(two)
        summary["2bed_avg"] = two["price"].mean() if len(two) else None
        text = (f"Studio: count={summary['studio_count']}, avg=${summary['studio_avg']:,.2f} | "
                f"2-bed: count={summary['2bed_count']}, avg=${summary['2bed_avg']:,.2f}")
        return text, pd.concat([studio.head(5), two.head(5)])

    return None, None

# Compose a natural/generative answer (optionally using OpenAI)
def synthesize_answer_with_context(query: str, retrieved_records: pd.DataFrame, use_openai: bool = False):
    # Build a short context snippet
    if len(retrieved_records) == 0:
        return "I couldn't find any properties relevant to your query."

    # create a short table snippet as context
    rows = []
    for i, r in retrieved_records.head(10).iterrows():
        rows.append({
            "address": str(r.get("address","")),
            "price": int(r.get("price",0)) if pd.notna(r.get("price")) else "",
            "bedrooms": int(r.get("bedrooms")) if pd.notna(r.get("bedrooms")) else "",
            "bathrooms": r.get("bathrooms",""),
            "description": (r.get("description","")[:200] + ("..." if len(str(r.get("description",""))) > 200 else ""))
        })
    context_text = "Properties (top results):\n" + "\n".join([f"- {row['address']} | ${row['price']} | {row['bedrooms']}bd/{row['bathrooms']}ba | {row['description']}" for row in rows])

    if use_openai and OPENAI_KEY:
        prompt = f"You are a helpful assistant answering this user question about properties. Use the context below to answer concisely and cite the properties by address.\n\nContext:\n{context_text}\n\nQuestion: {query}\n\nAnswer (short):"
        try:
            resp = openai.ChatCompletion.create(
                model="gpt-4o-mini", # change if you prefer older models; user can set key
                messages=[{"role":"user","content":prompt}],
                max_tokens=300,
                temperature=0.0
            )
            answer = resp["choices"][0]["message"]["content"].strip()
            return answer + "\n\nSources:\n" + "\n".join([f"- {r['address']} (${r['price']})" for _, r in retrieved_records.head(5).iterrows()])
        except Exception as e:
            st.warning(f"OpenAI call failed: {e}")
            # fallback to template
    # fallback: deterministic synthesis
    top = retrieved_records.head(5)
    avg_price = top["price"].mean() if "price" in top.columns else None
    answer_lines = [f"I found {len(retrieved_records)} matching records. Top {min(5,len(top))} results (address - price - beds/baths):"]
    for _, r in top.iterrows():
        address = r.get("address","<no address>")
        price = r.get("price","n/a")
        beds = r.get("bedrooms","n/a")
        baths = r.get("bathrooms","n/a")
        answer_lines.append(f"- {address} â€” ${price:,} â€” {beds} bd / {baths} ba")
    if avg_price:
        answer_lines.append(f"\nAverage price among top results: ${avg_price:,.2f}")
    answer_lines.append("\nSources: the dataset entries shown above (addresses listed).")
    return "\n".join(answer_lines)

# -------------------------
# Streamlit UI
# -------------------------
st.title("ðŸ˜ï¸ Property Data RAG â€” Streamlit Demo")

col1, col2 = st.columns([1, 3])

with col1:
    st.header("Data / Index")
    uploaded = st.file_uploader("Upload property CSV", type=["csv","json"])
    default_path = st.text_input("Or local CSV path (optional)", value="")
    if uploaded:
        try:
            df = pd.read_csv(uploaded) if uploaded.name.endswith(".csv") else pd.read_json(uploaded)
            st.success("File loaded from upload.")
        except Exception as e:
            st.error(f"Failed to load uploaded file: {e}")
            df = None
    elif default_path and os.path.exists(default_path):
        try:
            df = pd.read_csv(default_path)
            st.success(f"Loaded file from {default_path}")
        except Exception as e:
            st.error(f"Failed to load local file: {e}")
            df = None
    else:
        # try to use the sample path you included earlier if available
        sample_path = "/content/drive/MyDrive/RAG/Property_data.csv"
        if os.path.exists(sample_path):
            df = pd.read_csv(sample_path)
            st.info(f"Auto-loaded sample path: {sample_path}")
        else:
            df = None
            st.info("Upload a CSV or enter a local CSV path to get started.")

    if df is not None:
        st.write("Dataset snapshot:")
        st.dataframe(df.head(5))
        if st.button("Preprocess & Build Index"):
            with st.spinner("Preprocessing..."):
                df_proc = preprocess(df)
                model = get_embed_model()
                st.write("Creating embeddings (this may take a bit)...")
                descs = df_proc["description"].fillna("").astype(str).tolist()
                embeddings = embed_texts(model, descs)
                dim = embeddings.shape[1]
                index = build_faiss_index(embeddings, dim)
                # keep metadata aligned to index ids
                meta_list = df_proc.to_dict(orient="records")
                # save on disk
                save_index_and_meta(index, meta_list, filepath_index="faiss.index", filepath_meta="meta.pkl")
                # also save dataframe
                df_proc.to_csv("processed_properties.csv", index=False)
                st.success(f"Index built with {len(meta_list)} items and saved (faiss.index, meta.pkl).")

        if st.button("Load existing index (faiss.index + meta.pkl)"):
            idx, meta = load_index_and_meta()
            if idx is None:
                st.error("No saved index found in working dir.")
            else:
                st.success("Loaded FAISS index + metadata.")
                st.write(f"Index contains {len(meta)} items.")

with col2:
    st.header("Query / Explore")
    query = st.text_input("Ask about the properties (example: \"What's the average price of 3-bedroom homes?\")")
    use_openai = st.checkbox("Use OpenAI to synthesize answers (requires OPENAI_API_KEY)", value=False)

    # filters
    st.subheader("Filters (optional)")
    min_price, max_price = st.slider("Price range ($)", 0, 1_000_0000, (0, 1000000), step=5000)
    bedrooms_filter = st.selectbox("Bedrooms (Any / exact)", options=["Any"] + list(range(0,8)), index=0)
    min_bath = st.selectbox("Bathrooms (min)", options=["Any"] + list(range(0,6)), index=0)

    if st.button("Run Query"):
        idx, meta = load_index_and_meta()
        if idx is None or meta is None:
            st.error("Index not found. Please preprocess & build index first.")
        else:
            # load df from meta for analytic ops & filtering
            df_meta = pd.DataFrame(meta)
            df_meta = preprocess(df_meta)

            # apply simple filters before retrieval
            df_filtered = df_meta.copy()
            if min_price > 0 or max_price < 1_000_0000:
                df_filtered = df_filtered[(df_filtered["price"] >= min_price) & (df_filtered["price"] <= max_price)]
            if bedrooms_filter != "Any":
                df_filtered = df_filtered[df_filtered["bedrooms"] == int(bedrooms_filter)]
            if min_bath != "Any":
                df_filtered = df_filtered[df_filtered["bathrooms"].fillna(0) >= int(min_bath)]

            # first try rule-based analytic
            analytic_ans, analytic_df = handle_analytic_query(df_filtered, query)
            if analytic_ans:
                st.success("Analytic result")
                st.write(analytic_ans)
                if not analytic_df.empty:
                    st.dataframe(analytic_df.head(10))
            else:
                # RAG retrieval
                model = get_embed_model()
                q_emb = embed_texts(model, [query])
                # index was built on full dataset; we get nearest neighbors and then filter locally
                D, I = idx.search(q_emb, 20)  # retrieve top 20
                retrieved = []
                for idx_id in I[0]:
                    if idx_id < len(meta):
                        retrieved.append(meta[idx_id])
                retrieved_df = pd.DataFrame(retrieved)
                # apply the same filters to retrieved_df if user set filters
                if min_price > 0 or max_price < 1_000_0000:
                    retrieved_df = retrieved_df[(retrieved_df["price"] >= min_price) & (retrieved_df["price"] <= max_price)]
                if bedrooms_filter != "Any":
                    retrieved_df = retrieved_df[retrieved_df["bedrooms"] == int(bedrooms_filter)]
                if min_bath != "Any":
                    retrieved_df = retrieved_df[retrieved_df["bathrooms"].fillna(0) >= int(min_bath)]

                if len(retrieved_df) == 0:
                    st.warning("No results found after applying filters.")
                else:
                    st.write(f"Retrieved {len(retrieved_df)} candidate records (showing top 10):")
                    st.dataframe(retrieved_df.head(10)[["address","price","bedrooms","bathrooms","description"]])

                    answer = synthesize_answer_with_context(query, retrieved_df, use_openai and OPENAI_KEY is not None)
                    st.markdown("### Answer")
                    st.write(answer)
                    st.markdown("### Top matched properties (with similarity scores)")
                    # Show similarity scores from D
                    sim_scores = D[0][:len(retrieved_df)]
                    retrieved_df = retrieved_df.reset_index(drop=True)
                    retrieved_df["similarity"] = sim_scores
                    st.dataframe(retrieved_df.head(10)[["address","price","bedrooms","bathrooms","similarity","description"]])

st.markdown("---")
st.write("Notes:")
st.write("""
- This demo uses **sentence-transformers** + **FAISS** as a local vector DB. It's fully local and avoids paid vector DBs.
- If you want production-grade vector DBs, swap FAISS for Pinecone/Weaviate/Chroma and change the retrieval code accordingly.
- To use OpenAI to generate more fluent answers, set `OPENAI_API_KEY` in your environment and check the 'Use OpenAI' box.
- You can persist the index (`faiss.index`) and metadata (`meta.pkl`) in your project folder to avoid re-indexing.
""")